{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Praktyczny Machine Learning w Pythonie\n",
    "<br>\n",
    "<img src=\"figures/dilbert-2213.gif\">\n",
    "\n",
    "## Rozpoznawanie cyfr\n",
    "\n",
    "Nasz model będzie składał się z 2 kroków:\n",
    "1. Zmniejszenie ilości wymiarów\n",
    "2. Klasyfikacja modelem liniowym\n",
    "\n",
    "Najpierw jednak musimy zapoznać się ze zbiorem danych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_mldata\n",
    "mnist = fetch_mldata('MNIST original')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST\n",
    "\n",
    "<img src=\"figures/mnist_originals.png\">\n",
    "\n",
    "MNIST to baza danych odręcznie napisanych cyfr około 20 lat temu. Ludzie są w stanie rozpoznać ok. 99,5% cyfr z tego zbioru poprawnie.\n",
    "Zobaczymy ile nam się uda!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img = mnist.data[0]\n",
    "print \"Pierwsz obrazek z \", mnist.data.shape[0], \":\", img # Pixele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Możemy sobie narysować wcześniej wypisaną cyfrę\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "plt.imshow(img.reshape(28,28), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import linear_model, decomposition\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import preprocessing\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Krok 1: wczytanie i podzielenie danych\n",
    "\n",
    "Skalowanie jest bardzo ważne. Dzięki temu mamy średnią danej cechy 0 i wariancję 1 (mówiąc po ludzku, dane będą bardziej  przypominały kulke w przestrzeni wejściowej)\n",
    "\n",
    "<img src=\"figures/prepro2.jpeg\" width=600>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Wczytujemy dane i skalujemy\n",
    "X, Y = mnist.data.astype(\"float64\"), mnist.target \n",
    "X = preprocessing.scale(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tak jak wspomniałem algorytm trenujemy na innych danych niż testujemy. **To bardzo ważne**. Do każdych danych da się dopasować taki model, który idealnie na nich odpowiada (np. poprzez zapamiętanie wszystkich przykładów). Jeśli tak zrobimy, to mówimy że nasz model **zoverfitował**.\n",
    "\n",
    "<img src=\"figures/underfitting-overfitting.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Dzielimy na dane trenujące i testujące\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Krok 2: dopasujmy pare modeli - uwaga! To juz trwa chwile, 70000 przykladow o 784 cechach kazdy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Na wszystkich przykladach osiaga 92% dokladnosci. \n",
    "N = 500 # Podzbiór danych\n",
    "model = LogisticRegression() # Model z domyślnymi parametrami\n",
    "model.fit(X_train[0:N], Y_train[0:N])\n",
    "\n",
    "Y_test_predicted = model.predict(X_test)\n",
    "print \"Dokładność modelu wytrenowanego na \",N, \" przykladach to: \",100*sklearn.metrics.accuracy_score(Y_test, Y_test_predicted), \"%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(model.coef_[3].reshape(28,28), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(model.coef_[0].reshape(28,28), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Przyklad zaklasyfikowany jako \", model.predict(X_test[0])\n",
    "plt.imshow(X_test[0].reshape(28,28), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Krok 3: Tworzymy caly model zmniejszając ilość przykładów\n",
    "\n",
    "W scikit-learn możemy połączyć pare modeli w **pipeline**, który sam implementuje interfejs **Estimator**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca = decomposition.PCA(n_components=20)\n",
    "pca.fit(X_train)\n",
    "X_train_transf = pca.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logistic = linear_model.LogisticRegression(C=0.1)\n",
    "logistic.fit(X_train_transf, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline pozwala spinać obiekty typu `Estimator`. Za pomocą pipeline można tworzyć bardzo skomplikowane modele, które mają bardzo prosty interfejs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline(steps=[('pca', pca), ('logistic', logistic)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Zobaczmy na dokładność modelu\n",
    "Y_test_predicted = pipe.predict(X_test)\n",
    "print \"Dokładność modelu wytrenowanego to: \",100*sklearn.metrics.accuracy_score(Y_test, Y_test_predicted), \"%\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Poprawa o 6%! Nice**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Poszukajmy najlepszych hiperparametrów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "N = 200\n",
    "tuned_parameters = [{'pca__n_components':[10,20,30], 'logistic__C':  [1, 10, 100, 1000]}]\n",
    "clf_fitted = Pipeline(steps=[('pca', pca), ('logistic', logistic)])\n",
    "clf = GridSearchCV(clf_fitted, tuned_parameters, cv=5, scoring='accuracy', verbose=4)\n",
    "clf.fit(X_train[0:N], Y_train[0:N])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Zobaczmy na dokładność modelu\n",
    "Y_test_predicted = clf.best_estimator_.predict(X_test)\n",
    "print \"Dokładność modelu wytrenowanego to: \",100*sklearn.metrics.accuracy_score(Y_test, Y_test_predicted), \"%\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dokładność jest mniejsza, ponieważ ograniczyliśmy sztucznie ilość przykładów. Spróbuj ją zwiększyć"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Podobna metodologia zadziała na trudniejszych zbiorach danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_lfw_people\n",
    "\n",
    "# Wczytanie danych i rozmiarow\n",
    "lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)\n",
    "n_samples, h, w = lfw_people.images.shape\n",
    "\n",
    "# Podzial na zbior trenujacy i testujacy\n",
    "X = lfw_people.data\n",
    "Y = lfw_people.target\n",
    "X = preprocessing.scale(X.astype(\"float64\")) #scale() to preprocessing który normalizuje dane ze sa symetrycznie wokol 0\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y)\n",
    "\n",
    "\n",
    "plt.imshow(X_train[2].reshape(h,w), cmap=plt.cm.gray)\n",
    "logistic = linear_model.LogisticRegression(C=1)\n",
    "logistic.fit(X_train, Y_train)\n",
    "pipe = Pipeline(steps=[('pca', pca), ('logistic', logistic)])\n",
    "Y_test_predicted = logistic.predict(X_test)\n",
    "print \"Dokładność modelu wytrenowanego to: \",100*sklearn.metrics.accuracy_score(Y_test, Y_test_predicted), \"%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_samples, h, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## TODO: Dodatkowa sekcja?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "\n",
    "Jak widać, na rozważanych zbiorach, można osiągnąć wysoką dokładność (>85%) używając prostych modeli. Niestety na bardziej skomplikowanych danych (np. CIFAR-100, https://www.cs.toronto.edu/~kriz/cifar.html) nie jest już tak prosto.\n",
    "\n",
    "Tradycyjne architektury, które były popularne do niedawna, używały ręcznej ekstrakcji \"lepszych\" cech:\n",
    "\n",
    "<img src=\"figures/standard.jpg\">\n",
    "\n",
    "Deep Learning jest poddziedziną machine learningu, gdzie rozważamy architektury, które samoistnie uczą się cech na tyle dobrych, że dokładnie takie same modele jak Państwu pokazywaliśmy działają z zadowalającą precyzją\n",
    "\n",
    "<img src=\"figures/features.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sieci konwolucyjne,\n",
    "### i co to ma wspólnego z AlphaGo?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cnn.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jak może wyglądać taka architektura?\n",
    "\n",
    "Zastanówmy się jakbyśmy wykrywali krawędź na obrazku? Można to zrealizować przez \"przykładanie\" macierzy\n",
    "\n",
    "```\n",
    "0 0 0\n",
    "1 1 1\n",
    "0 0 0\n",
    "```\n",
    "\n",
    "do obrazka w każdym miejscu, taką operację nazywamy konwolucją. W wyniku dostaniemy obrazek, gdzie aktywnością pixela jest jego zgodność (i jego otoczenia) z takim obrazkiem krawędzi (filtrem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
